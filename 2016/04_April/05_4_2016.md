#Running to do list
(somehow this keeps getting longer and not shorter)
+ Add MuTect2 into Loeb Snakemake workflow
+ Check MuTect2 output for 100% Tumor Colo Sample, and create ROC curve.
    + Work on code to generate the FM/FP plots.
    + Create code to generate ROC curves overall
+ Run cutadapt prior to barcode collapse with BMFtools, and compare to run where cutadapt comes after collapse.
+ Run translocation pipeline on the ctDNA sample
+ Create analysis snakemake workflow
+ Run analysis pipeline for IDT Inline HiSeq Run
+ Run collapse and alignment pipeline on the Swift data (**done for all but NA12878**)
+ Carefully read CAPP-Seq paper, identify useful methods, assess our current approach.
+ Run new Inline barcode samples through MuTect2, in particular the new normals, etc.
+ re-run Strelka on these samples as well, with the new bed file.

##Locating Missing Reads
Right now we are observing significantly lower coverage than we had in the past.  With a post-collapse depth of rough ~300-600x.  Rather than the ~1200-1600x we had been observing previously.  It is unclear at this point whether we were overestimating before, or understimating now.  To double check that the reads were demultiplexed off the sequencer properly, I made a brief shell scrip to count the reads in each FASTQ set and sum them.

    #!/bin/bash
    Files="*R1_001.fastq.gz"
    for f in $Files
    do
    	base=`echo $f | sed 's/R1.*//g'`
    	R1=$base\R1_001.fastq.gz
    	R2=$base\R2_001.fastq.gz
    	reads1=`zcat $R1 | wc -l &`
    	reads2=`zcat $R2 | wc -l`
    	totalR=`echo $reads1\/4 + $reads2\/4 | bc`
    	echo $base $totalR
    done

This allows us to make sure that we observe all of the reads we should before running any bioinformatic analysis.

Which we did:

| 12316R_Inline_IDT_HiSe           |            |        |
|----------------------------------|------------|--------|
| K1-100Colo-IDT-INL_S1_           | 115435354  | 14.13% |
| K2-5Colo-95BL-IDT-INL_S2_        | 75689062   | 9.27%  |
| K3-05Colo-995BL-IDT-INL_S3_      | 58707452   | 7.19%  |
| K4-100BL-IDT-INL_S4_             | 135386316  | 16.58% |
| K5-NA12878-IDT-INL_S5_           | 96181120   | 11.78% |
| K6-150000390-Buffy-IDT-INL-_S6_  | 74979274   | 9.18%  |
| K7-150000389-cfDNA-IDT-INL_S7_   | 85749686   | 10.50% |
| K8-Normal-cfDNA-222-IDT-INL_S8_  | 69541402   | 8.51%  |
| K9-Normal-cfDNA-224-IDT-INL_S9_  | 76708096   | 9.39%  |
| Undetermined_S0_                 | 28320104   | 3.47%  |
| Total Reads                      | 816697866  |        |
| Read Pairs                       | 408348933  |        |
|                                  |            |        |
| 12318R_9N_Swift_HiSeq            |            |        |
| S1-100Colo-SWIFT_S1_             | 115693398  | 9.29%  |
| S2-5Colo-95BL-SWIFT_S2_          | 160780266  | 12.91% |
| S3-05Colo-995BL-SWIFT_S3_        | 194207070  | 15.59% |
| S4-100BL-SWIFT_S4_               | 178691076  | 14.35% |
| S5-NA12878-SWIFT_S5_             | 165280212  | 13.27% |
| S6-150000390-Buffy-SWIFT_S6_     | 89669376   | 7.20%  |
| S7-150000389-cfDNA-SWIFT_S7_     | 118538454  | 9.52%  |
| S8-Normal-cfDNA-222-SWIFT_S8_    | 75562209   | 6.07%  |
| S9-Normal-cfDNA-224-SWIFT_S9_    | 106354827  | 8.54%  |
| Undetermined_S0_                 | 40749618   | 3.27%  |
| Total Reads (including index fq) | 1245526506 |        |
| Read Sets (R1, R2, R3)           | 622763253  |        |

So, at least I can be confident that all of the reads are present before running the pipeline.

The script to do the counting lives:

    /uufs/chpc.utah.edu/common/home/arup-storage1/u0521163/ctDNA_Data/12316R_Inline_IDT_HiSeq/FASTQ/fq_counter.sh

To follow this, I'm now going to run the Colo100 data from both the Swift_9N and the IDT_Inline sets by hand, and try to account for the reads at each step.  I will track my command lines for this in the headers below.

##Swift_9N Secondary Index by hand

    /home/brett/projects/BMFTools_Devel/Experimental_Data/HiSeq_9N_Swift_12318R/Manual_Test

    Step by step with no pipes.
    Number of reads in original FASTQ: 115693398

####Step 1: Collapse by Barcode

    bmftools sdmp -zo S1_temp -t 7 -i S1-100Colo-SWIFT_S1_R2_001.fastq.gz -s 3 -d -p 12 -c -f S1-100Colo-SWIFT_S1 S1-100Colo-SWIFT_S1_R1_001.fastq.gz S1-100Colo-SWIFT_S1_R3_001.fastq.gz

    lines after collapse: 17142412
    reads after collapse: 8571206
    expected mean family size: 13.49791


####Step 2: cutadapt

    cutadapt --mask-adapter -O 6 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -g ACACTCTTTCCCTACACGACGCTCTTCCGATCT -A AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -G ACACTCTTTCCCTACACGACGCTCTTCCGATCT -o S1-100Colo-SWIFT.ca.R1.fq.gz -p S1-100Colo-SWIFT.ca.R2.fq.gz S1-100Colo-SWIFT_S1.R1.fq.gz S1-100Colo-SWIFT_S1.R2.fq.gz

I wonder if i have to specify the --no-trim?

output:

    Total read pairs processed:          4,285,603
        Read 1 with adapter:                 820,580 (19.1%)
        Read 2 with adapter:                 312,248 (7.3%)
    Pairs written (passing filters):     4,285,603 (100.0%)

I am missing no reads after this step:

    zcat S1-100Colo-SWIFT.ca.R1.fq.gz | wc -l
    17142412
    zcat S1-100Colo-SWIFT_S1.R1.fq.gz | wc -l
    17142412

So cutadapt isn't dropping any reads.

Reads = 17142412/2 = 8571206

*Note: I'm just going to continue this document on Apr 6th, so this is all in one set of notes*
####Step 3: alignment

    bwa mem -YCT 0 -R '@RG\tID:S1-100Colo-SWIFT\tSM:S1-100Colo-SWIFT\tPL:Illumina' -t 10 /home/brett/projects/Pipeline_Resrouces/resources/resources/usr/dbs/human_g1k_v37.fasta S1-100Colo-SWIFT.ca.R1.fq.gz S1-100Colo-SWIFT.ca.R2.fq.gz | samtools sort -O bam -T S1_temp > S1-100Colo-SWIFT.bam

number of primary alignment reads after alignment:

    samtools view -F 2816 -c S1-100Colo-SWIFT.bam
    8571206

Looks like bwa is dumping a few reads, but nothing drastic.  

Looks like at this point samtools sort was stripping the RG tag from  the header, causing some reads to fail QC and to possibly get filtered out.  I'm re-installing samtools 1.3 and running from there.  See if that recoveres some problems.  I should note, this does not appear to be a problem on the chpc, where the RG and PG tags are preserved into the final bams.  *notes: the problem was solved and now everything works okay for the RG tag*

the above command line is wrong.  **CANNOT samtools sort before mark***.  Mark must be done after the alignment, before any sorting.  Should just be run as:

    bwa mem -YCT 0 -R '@RG\tID:S1-100Colo-SWIFT\tSM:S1-100Colo-SWIFT\tPL:Illumina' -t 10 /home/brett/projects/Pipeline_Resrouces/resources/resources/usr/dbs/human_g1k_v37.fasta S1-100Colo-SWIFT.ca.R1.fq.gz S1-100Colo-SWIFT.ca.R2.fq.gz > S1-100Colo-SWIFT.sam

    samtools view -F 2816 -c S1-100Colo-SWIFT.sam
    8571206

####Step 4: Mark

**Try 1**

    bmftools mark S1-100Colo-SWIFT.bam S1-100Colo-SWIFT.mark.bam
    samtools view -c S1-100Colo-SWIFT.mark.bam
    8531236

Looks like mark is losing some reads. Daniel will investigate.

**Try 2**

    bmftools mark -l 4 S1-100Colo-SWIFT.sam S1-100Colo-SWIFT.mark.bam

    samtools view -F 2816 -c S1-100Colo-SWIFT.mark.bam
    8483984

We now appear to be losing a considerable number of reads at the mark step, 8571206 - 8483984 =  87222 reads lost.  There should not be any reads lost ast this step.

####Step 5: BMFtools sort (ucs)

**Try 1**

*This is broken because I samtools sorted before marking*

    bmftools sort -m 1G -@ 8 -k ucs -O bam -T temp_colo S1-100Colo-SWIFT.mark.bam > S1-100Colo-SWIFT.mark.sort.bam
    samtools view -F 2816 -c S1-100Colo-SWIFT.mark.sort.bam
    8531236

**Try 2**

    bmftools sort -m 1G -@ 8 -k ucs -O bam -T temp_colo S1-100Colo-SWIFT.mark.bam > S1-100Colo-SWIFT.mark.sort.bam
    samtools view -F 2816 -c S1-100Colo-SWIFT.mark.sort.bam
    8483984


####Step 6: BMFtools Rescue

    bmftools rsq -uf temp_rescued.fastq -l 4 S1-100Colo-SWIFT.mark.sort.bam S1-100Colo-SWIFT.mark.sort.rsqRemoved.bam
    samtools view -F 2816 -c S1-100Colo-SWIFT.mark.sort.rsqRemoved.bam
    4275537
    wc -l temp_rescued.fastq
    2741496

    Total reads = 4275537 + 2741496 = 7017033

Tons of reads being rescued, **7017033/8531236 = 0.1774893**, so almost **18%** of reads are being rescued!! That's way too high.

Daniel has tracked down the issue to a bug in his program, we are rescuing far too many reads.  It appears that one of his functions is not working properly, he didn't go into detail.

####Step 6 Redo: BMFtools Rescue
Daniel pushed the changes to Dev.  These runs will follow from the **Try 2** outputs in previous steps.

    bmftools rsq -uf temp_rescued.fastq -l 4 S1-100Colo-SWIFT.mark.sort.bam S1-100Colo-SWIFT.mark.sort.rsqRemoved.bam
    samtools view -F 2816 -c S1-100Colo-SWIFT.mark.sort.rsqRemoved.bam
    7173430
    wc -l temp_rescued.fastq
    2558832
    echo 2558832/4 | bc
    639708

In this run we had 639708 reads rescued, making the total fraction 1 - 7173430/8483984 = 0.1544739, which is still much higher than I'd expect.  **15% reads recovered is too many**.
